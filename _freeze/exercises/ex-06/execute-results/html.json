{
  "hash": "a885d64397d48c7f3e446de69ce6afb7",
  "result": {
    "markdown": "---\ntitle: 'Ex-07: Classifying Spam Emails'\n---\n\n## Objective:\n\nApply and evaluate different classification models to predict high-risk credit individuals using financial traits, focusing on understanding model performance through various evaluation metrics.\n\n**Prerequisites:** Ensure you have Python, Jupyter Notebook, and the required libraries (**`pandas`**, **`numpy`**, **`scikit-learn`**, **`matplotlib`**, **`seaborn`**, **`mord`**) installed. The dataset **`spam.csv`** should be available in the **`data`** directory.\n\n## Dataset:\n\nThe data this week comes from Vincent Arel-Bundock's Rdatasets package(<https://vincentarelbundock.github.io/Rdatasets/index.html>).\n\n> Rdatasets is a collection of 2246 datasets which were originally distributed alongside the statistical software environment R and some of its add-on packages. The goal is to make these data more broadly accessible for teaching and statistical software development.\n\nWe're working with the [spam email](https://vincentarelbundock.github.io/Rdatasets/doc/DAAG/spam7.html) dataset. This is a subset of the [spam e-mail database](https://search.r-project.org/CRAN/refmans/kernlab/html/spam.html).\n\nThis is a dataset collected at Hewlett-Packard Labs by Mark Hopkins, Erik Reeber, George Forman, and Jaap Suermondt and shared with the [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/94/spambase). The dataset classifies 4601 e-mails as spam or non-spam, with additional variables indicating the frequency of certain words and characters in the e-mail.\n\n### Metadata\n\n| Variable  | Class     | Description                                                              |\n|-----------|-----------|--------------------------------------------------------------------------|\n| `crl.tot` | double    | Total length of uninterrupted sequences of capitals                      |\n| `dollar`  | double    | Occurrences of the dollar sign, as percent of total number of characters |\n| `bang`    | double    | Occurrences of `!`, as percent of total number of characters             |\n| `money`   | double    | Occurrences of `money`, as percent of total number of characters         |\n| `n000`    | double    | Occurrences of the string `000`, as percent of total number of words     |\n| `make`    | double    | Occurrences of `make`, as a percent of total number of words             |\n| `yesno`   | character | Outcome variable, a factor with levels `n` not spam, `y` spam            |\n\n(Source: [TidyTuesday](https://github.com/rfordatascience/tidytuesday/blob/master/data/2023/2023-08-15/readme.md))\n\n## Question:\n\nCan we predict whether an email is Spam or not using decision tree classification?\n\n## **Step 1: Setup and Data Preprocessing**\n\n-   Start by importing the necessary libraries and load the **`spam.csv`** dataset.\n\n-   Preprocess the data by encoding categorical variables, defining features and target, and splitting the data into training and testing sets. Finally, apply PCA to reduce dimensionality.\n\n\n    ::: {.cell execution_count=1}\n    ``` {.python .cell-code}\n    # Import libraries\n    import pandas as pd\n    import numpy as np\n    from sklearn.preprocessing import LabelEncoder\n    from sklearn.decomposition import PCA\n    from sklearn.model_selection import train_test_split\n    \n    # Load the dataset\n    spam = pd.read_csv(\"data/spam.csv\")\n    \n    # Encode categorical variables\n    categorical_columns = spam.select_dtypes(include = ['object', 'category']).columns.tolist()\n    label_encoders = {col: LabelEncoder() for col in categorical_columns}\n    for col in categorical_columns:\n        spam[col] = label_encoders[col].fit_transform(spam[col])\n    \n    # Define features and target\n    X = spam.drop('yesno', axis = 1)\n    y = spam['yesno']\n    \n    # Split the data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n    \n    # Reduce dimensionality\n    pca = PCA(n_components = 2)\n    X_train_pca = pca.fit_transform(X_train)\n    X_test_pca = pca.transform(X_test)\n    ```\n    :::\n    \n    \n## **Step 2: Model Training and Decision Boundary Visualization**\n\n-   Train a Decision Tree classifier on the PCA-transformed training data.\n\n-   Implement and use the **`decisionplot`** function to visualize the decision boundary of your trained model.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.tree import DecisionTreeClassifier\nimport matplotlib.pyplot as plt\n\n# Train Decision Tree\ndtree = DecisionTreeClassifier()\ndtree.fit(X_train_pca, y_train)\n\n# Implement the decisionplot function (as provided in the lecture content)\n# Add the decisionplot function here\n\n# Visualize decision boundary\ndecisionplot(dtree, pd.DataFrame(X_train_pca, columns = ['PC1', 'PC2']), y_train)\n```\n:::\n\n\n## **Step 3: Model Evaluation**\n\n-   Evaluate your model using accuracy, precision, recall, F1 score, and AUC-ROC metrics.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\n\n# Predictions\npredictions = dtree.predict(X_test_pca)\n\n# Evaluate metrics\naccuracy = accuracy_score(y_test, predictions)\nprecision = precision_score(y_test, predictions, average = 'weighted')\nrecall = recall_score(y_test, predictions, average = 'weighted')\nf1 = f1_score(y_test, predictions, average = 'weighted')\n\n# Display results\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall: {recall:.2f}\")\nprint(f\"F1 Score: {f1:.2f}\")\n\n# For AUC-ROC, binarize the output and calculate AUC-ROC for each class\n# Add the necessary code for AUC-ROC calculation here (refer to lecture content)\n```\n:::\n\n\n## **Assignment:**\n\n-   Implement the missing parts of the code: the **`decisionplot`** function and AUC-ROC calculation.\n\n-   Discuss the results among your peers. Consider the following:\n\n    -   Which metric is most informative for this problem and why?\n\n    -   How does the decision boundary visualization help in understanding the model's performance?\n\n    -   Reflect on the impact of PCA on model performance and decision boundary.\n\n## **Submission:**\n\n-   Submit your Jupyter Notebook via GitHub with implemented code and a brief summary of your discussion findings regarding model evaluation and the impact of PCA.\n\n",
    "supporting": [
      "ex-06_files"
    ],
    "filters": [],
    "includes": {}
  }
}