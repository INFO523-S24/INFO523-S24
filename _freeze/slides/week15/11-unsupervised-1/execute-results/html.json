{
  "hash": "bf4f06d9d6da38cfcbe8051d09620159",
  "result": {
    "markdown": "---\ntitle: Unsupervised<br>Learning I\nsubtitle: Lecture 11\ntitle-slide-attributes:\n  data-background-image: ../minedata-bg.png\n  data-background-size: 600px, cover\n  data-slide-number: none\nformat: revealjs\nexecute: \n  warning: false\n  message: false\n  error: false\nauto-stretch: false\n---\n\n# Warm up\n\n## Announcements\n\n-   HW 05 is due Fri Apr 26, 11:59pm\n\n## Setup {.smaller}\n\n::: {#setup .cell execution_count=1}\n``` {.python .cell-code}\n# Data Handling and Manipulation\nimport pandas as pd\nimport numpy as np\n\n# Data Preprocessing\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.decomposition import PCA\n\n# Model Selection and Evaluation\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\nfrom sklearn.mixture import GaussianMixture\n\n# Machine Learning Models\nfrom sklearn.cluster import KMeans\nfrom sklearn_extra.cluster import KMedoids\n\n# Data Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set the default style for visualization\nsns.set_theme(style = \"white\", palette = \"colorblind\")\n\n# Increase font size of all Seaborn plot elements\nsns.set(font_scale = 1.25)\n```\n:::\n\n\n# Unsupervised Learning\n\n## \n\n<br>\n\n![Credit: [Recro](https://recro.io/blog/supervised-vs-unsupervised-learning-key-differences/)](images/unsupervised.jpeg){fig-align=\"center\" width=\"1528\"}\n\n## Clustering\n\n![](images/clustering-1.png){fig-align=\"center\"}\n\n## Clustering {.smaller}\n\nSome use cases for clustering include:\n\n::: incremental\n-   [**Recommender systems**](https://pages.dataiku.com/recommendation-engines):\n\n    -   Grouping together users with similar viewing patterns on Netflix, in order to recommend similar content\n\n-   [**Anomaly detection**](https://pages.dataiku.com/anomaly-detection-at-scale-guidebook):\n\n    -   Fraud detection, detecting defective mechanical parts\n\n-   **Genetics**:\n\n    -   Clustering DNA patterns to analyze evolutionary biology\n\n-   **Customer segmentation**:\n\n    -   Understanding different customer segments to devise marketing strategies\n:::\n\n## Question:\n\nCan we [identify distinct baseball player groupings]{.underline} based on their **player stats** in 2018?\n\n## Our data: MLB player stats {.smaller}\n\n::: panel-tabset\n## Read + Head\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nmlb_players_18 = pd.read_csv(\"data/mlb_players_18.csv\", encoding = 'iso-8859-1')\n\nmlb_players_18.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>team</th>\n      <th>position</th>\n      <th>games</th>\n      <th>AB</th>\n      <th>R</th>\n      <th>H</th>\n      <th>doubles</th>\n      <th>triples</th>\n      <th>HR</th>\n      <th>RBI</th>\n      <th>walks</th>\n      <th>strike_outs</th>\n      <th>stolen_bases</th>\n      <th>caught_stealing_base</th>\n      <th>AVG</th>\n      <th>OBP</th>\n      <th>SLG</th>\n      <th>OPS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Allard, K</td>\n      <td>ATL</td>\n      <td>P</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Gibson, K</td>\n      <td>MIN</td>\n      <td>P</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Law, D</td>\n      <td>SF</td>\n      <td>P</td>\n      <td>7</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Nuno, V</td>\n      <td>TB</td>\n      <td>P</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Romero, E</td>\n      <td>KC</td>\n      <td>P</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>3.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Metadata\n\n::: {style=\"text-align: center;\"}\n\n```{=html}\n<iframe width=\"1200\" height=\"400\" src=\"https://openintrostat.github.io/openintro/reference/mlb_players_18.html\" frameborder=\"1\" style=\"background:white;\"></iframe>\n```\n\n:::\n\n## Info\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nmlb_players_18.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1270 entries, 0 to 1269\nData columns (total 19 columns):\n #   Column                Non-Null Count  Dtype  \n---  ------                --------------  -----  \n 0   name                  1270 non-null   object \n 1   team                  1270 non-null   object \n 2   position              1270 non-null   object \n 3   games                 1270 non-null   int64  \n 4   AB                    1270 non-null   int64  \n 5   R                     1270 non-null   int64  \n 6   H                     1270 non-null   int64  \n 7   doubles               1270 non-null   int64  \n 8   triples               1270 non-null   int64  \n 9   HR                    1270 non-null   int64  \n 10  RBI                   1270 non-null   int64  \n 11  walks                 1270 non-null   int64  \n 12  strike_outs           1270 non-null   int64  \n 13  stolen_bases          1270 non-null   int64  \n 14  caught_stealing_base  1270 non-null   int64  \n 15  AVG                   1270 non-null   float64\n 16  OBP                   1270 non-null   float64\n 17  SLG                   1270 non-null   float64\n 18  OPS                   1270 non-null   float64\ndtypes: float64(4), int64(12), object(3)\nmemory usage: 188.6+ KB\n```\n:::\n:::\n\n\n## Categories\n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\"}\n# Assign data\ndf = mlb_players_18\n\n# Select categorical columns\ncategorical_cols = df.columns\n\n# Initialize a dictionary to store results\ncategory_analysis = {}\n\n# Loop through each categorical column\nfor col in categorical_cols:\n    counts = df[col].value_counts()\n    proportions = df[col].value_counts(normalize=True)\n    unique_levels = df[col].unique()\n    \n    # Store results in dictionary\n    category_analysis[col] = {\n        'Unique Levels': unique_levels,\n        'Counts': counts,\n        'Proportions': proportions\n    }\n\n# Print results\nfor col, data in category_analysis.items():\n    print(f\"Analysis for {col}:\\n\")\n    print(\"Unique Levels:\", data['Unique Levels'])\n    print(\"\\nCounts:\\n\", data['Counts'])\n    print(\"\\nProportions:\\n\", data['Proportions'])\n    print(\"\\n\" + \"-\"*50 + \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis for name:\n\nUnique Levels: [' Allard, K' ' Gibson, K' ' Law, D' ... ' Zamora, D' ' Zastryzny, R'\n ' Ziegler, B']\n\nCounts:\n name\n Anderson, T    3\n Garcia, J      3\n Guerra, J      3\n Sanchez, A     3\n Santana, D     3\n               ..\n Taylor, M      1\n Bour, J        1\n Flowers, T     1\n Davidson, M    1\n Ziegler, B     1\nName: count, Length: 1224, dtype: int64\n\nProportions:\n name\n Anderson, T    0.002362\n Garcia, J      0.002362\n Guerra, J      0.002362\n Sanchez, A     0.002362\n Santana, D     0.002362\n                  ...   \n Taylor, M      0.000787\n Bour, J        0.000787\n Flowers, T     0.000787\n Davidson, M    0.000787\n Ziegler, B     0.000787\nName: proportion, Length: 1224, dtype: float64\n\n--------------------------------------------------\n\nAnalysis for team:\n\nUnique Levels: ['ATL' 'MIN' 'SF' 'TB' 'KC' 'CHC' 'MIL' 'PIT' 'SEA' 'NYM' 'CWS' 'COL'\n 'LAD' 'BOS' 'MIA' 'NYY' 'TOR' 'WSH' 'LAA' 'OAK' 'TEX' 'HOU' 'CLE' 'CIN'\n 'PHI' 'STL' 'DET' 'ARI' 'BAL' 'SD']\n\nCounts:\n team\nATL    53\nNYM    51\nLAD    50\nMIL    49\nCHC    48\nARI    47\nCIN    46\nSF     45\nSTL    45\nWSH    45\nMIA    44\nDET    44\nPHI    43\nTOR    43\nSD     43\nLAA    42\nCOL    41\nOAK    41\nBAL    41\nBOS    40\nSEA    40\nPIT    40\nTB     39\nTEX    38\nCLE    38\nKC     38\nNYY    38\nHOU    34\nMIN    32\nCWS    32\nName: count, dtype: int64\n\nProportions:\n team\nATL    0.041732\nNYM    0.040157\nLAD    0.039370\nMIL    0.038583\nCHC    0.037795\nARI    0.037008\nCIN    0.036220\nSF     0.035433\nSTL    0.035433\nWSH    0.035433\nMIA    0.034646\nDET    0.034646\nPHI    0.033858\nTOR    0.033858\nSD     0.033858\nLAA    0.033071\nCOL    0.032283\nOAK    0.032283\nBAL    0.032283\nBOS    0.031496\nSEA    0.031496\nPIT    0.031496\nTB     0.030709\nTEX    0.029921\nCLE    0.029921\nKC     0.029921\nNYY    0.029921\nHOU    0.026772\nMIN    0.025197\nCWS    0.025197\nName: proportion, dtype: float64\n\n--------------------------------------------------\n\nAnalysis for position:\n\nUnique Levels: ['P' 'C' 'SS' 'RF' '3B' 'CF' 'LF' '2B' '1B' 'DH']\n\nCounts:\n position\nP     642\nC     115\n2B     83\nCF     79\nRF     76\n3B     73\nLF     70\nSS     67\n1B     59\nDH      6\nName: count, dtype: int64\n\nProportions:\n position\nP     0.505512\nC     0.090551\n2B    0.065354\nCF    0.062205\nRF    0.059843\n3B    0.057480\nLF    0.055118\nSS    0.052756\n1B    0.046457\nDH    0.004724\nName: proportion, dtype: float64\n\n--------------------------------------------------\n\nAnalysis for games:\n\nUnique Levels: [  3   1   7   4  43  14  65   6  56  60  52  10  45   2   5  29 136  30\n  54   9 150  63 147  47 137  23 103 140  39 154 162 143 141 111 139 152\n 158 144  80  40  95  31  13 135  89  91  38 157 149 156 113  55 132  18\n 146  51 116 160 148  48  21  77 138  59  82  19 104 145 142 105  24  76\n  25 153 127  78  41  87 112  85 109 125 108  75  66  28 128  97 129  22\n 119 102 101 151 134 123 131  68  16  26  36 126  17  15  81  61  20 130\n  44  35  90  32  37  50 117  49  73 110 107 155 159  84  42  57  83 161\n  98  74 114 133  11 122  92  86  94  58 124  79  33  96 115 121  46  67\n  93  69 120  34  88   8 100 118  72  62  27 106  12  71  70  53  64]\n\nCounts:\n games\n1      96\n2      76\n3      70\n4      54\n5      31\n       ..\n150     1\n120     1\n93      1\n135     1\n98      1\nName: count, Length: 161, dtype: int64\n\nProportions:\n games\n1      0.075591\n2      0.059843\n3      0.055118\n4      0.042520\n5      0.024409\n         ...   \n150    0.000787\n120    0.000787\n93     0.000787\n135    0.000787\n98     0.000787\nName: proportion, Length: 161, dtype: float64\n\n--------------------------------------------------\n\nAnalysis for AB:\n\nUnique Levels: [  1   2   3   6   4   7   5   8  93 520  59   9 569 225 574 143  84 534\n  70 365 471 109 584 618 570 529 539 382 455 632 586 310 152 319  10  60\n 487 504 281 328 134 620 623 573 590 280 480 503 433  58  99 140 560 123\n 554 171 414 598 626 537 606  31 579 593 462 513 156 302 559 178 594 444\n 252  63 326 596 401 398  74 215  53 527 566 486 249 580 582 250 100 261\n 165 413 547 288 321 415 661 318 477 463 293 275 210 533 494  40 284 492\n  11 396  66 389 257 464 151 431 258 664 347 440 296 578  26 432 556 546\n 101 116 176 386  15 405 169 617 403 211 283  34 136 544 597 223 499 174\n 106 379 395  19 190 422 255 141 282 351 149 344  65 501 459 639 356 437\n 330 557  50  77 466 131  27 332 170 236 298 489 564 265 519 402 407 380\n 550 110 531 177 358 130 524 461 536  75 312 474 316 399 605 613 218 119\n 488 512 294 306 394 199 335 467 516  12 420 353 438 342 460 230 206 202\n 602  81 576 126 500 187  61 558 424 465 506 404 213 242 197 543  33 343\n 600 505 498 436  25 450 567 417 476 117 184  46 159 478 428 235 349 139\n  76 203 292 555 530 348 429  68 493 510 387 128  64 278  30 133  73 146\n 129  86 473  43 357 181 125 371 303 485  13 308 408 369 244 362 192 166\n 434 251 423 468 221 115 452  71 373 120 196 299 532 360  18 207  36 212\n 583 113 427 241 191 224  96  32 247  23 161  69 189 194  37 334 553  14\n 164  94  57 238  67  91 182  48 491  29  87 214 200  44 103 325 208 323\n 195  20 266 272  56 384  41 289 147 163 228  16  54  92  55 160 243 111\n  78  28 118  45 135  79  17 102 470 179  72  90  24 150  49  38 122  42\n  21  22  97 112  39  47  52  35  51  62   0]\n\nCounts:\n AB\n0      286\n1       92\n2       64\n3       35\n4       28\n      ... \n335      1\n467      1\n420      1\n438      1\n605      1\nName: count, Length: 389, dtype: int64\n\nProportions:\n AB\n0      0.225197\n1      0.072441\n2      0.050394\n3      0.027559\n4      0.022047\n         ...   \n335    0.000787\n467    0.000787\n420    0.000787\n438    0.000787\n605    0.000787\nName: proportion, Length: 389, dtype: float64\n\n--------------------------------------------------\n\nAnalysis for R:\n\nUnique Levels: [  1   2   0   9 129   4 111  35 118  30  11  84  10  62 101  15  86  94\n  89  88  90  39  67  64  91  44  17  55   3  65  38  40  78  83 104  26\n  59  75  68  21  77 119  70   5 103  95  72  27   8  87  85  19 105  33\n  52  47  79  74  54  69  28  36  76 100  43  81  41  49  71 102  80  31\n  50  37  63  23  45  48 110  13  60   7  56  22  61  25  20  46  53  16\n  29  82  14  66  42  32  12  34  98  57  51  73  24   6  18]\n\nCounts:\n R\n0      562\n1       83\n2       52\n3       28\n4       22\n      ... \n98       1\n57       1\n118      1\n100      1\n104      1\nName: count, Length: 105, dtype: int64\n\nProportions:\n R\n0      0.442520\n1      0.065354\n2      0.040945\n3      0.022047\n4      0.017323\n         ...   \n98     0.000787\n57     0.000787\n118    0.000787\n100    0.000787\n104    0.000787\nName: proportion, Length: 105, dtype: float64\n\n--------------------------------------------------\n\nAnalysis for H:\n\nUnique Levels: [  1   2   4   3  33 180  20 188  74 187  46  27 169  22 114 147  34 181\n 191 176 163 166 117 139 192 178  94  96  18 146 151  84  98  40 185 170\n 175  83  45 142 148 127  17  29  41 164  36 162  50 121 174 182 156   9\n 168 172 134  87 161  51  72  93 143 113  21  61  15 149 160 137  70  28\n  73 115 165 152  80 155  89 183  88 132 128  81  76  58  16 136  11  77\n  78 135  68 108 118 106 154 126 119  42   7 116 141  31  47 103 159 107\n  56  75 144 158  59  14 100 104   5 111  67  37  92  39  90 131 120 167\n  86 145  13  44 133 101  97 140  91  19  79 153  55  30 123 129  99 105\n  10 109  85  57 124  52  48   8 122   6  38 102  69 125  82  65 110 112\n  71  23  24  26  53  43  32  49  54  25  35  12  66  60   0]\n\nCounts:\n H\n0      501\n1       78\n3       41\n2       39\n4       30\n      ... \n151      1\n191      1\n140      1\n153      1\n187      1\nName: count, Length: 177, dtype: int64\n\nProportions:\n H\n0      0.394488\n1      0.061417\n3      0.032283\n2      0.030709\n4      0.023622\n         ...   \n151    0.000787\n191    0.000787\n140    0.000787\n153    0.000787\n187    0.000787\nName: proportion, Length: 177, dtype: float64\n\n--------------------------------------------------\n\nAnalysis for doubles:\n\nUnique Levels: [ 0  1  2  4 47 37 11 34  5  6 29  9 31 24 30 44 36 25 22 28 43 14 18 33\n 35 16 15 46 38 12 27 26 10 42 40  3 41 45 51 21 17  8  7 32 19 23 13 48\n 20]\n\nCounts:\n doubles\n0     653\n1      94\n2      46\n3      30\n4      26\n5      24\n6      22\n14     21\n18     20\n9      18\n13     17\n8      16\n23     16\n10     16\n16     15\n11     15\n12     15\n22     15\n25     14\n7      14\n15     13\n28     12\n17     12\n26     10\n21      9\n19      9\n27      8\n20      8\n31      8\n35      7\n30      7\n32      7\n33      6\n34      6\n29      6\n24      6\n38      4\n42      4\n36      4\n40      3\n41      2\n44      2\n43      2\n37      2\n47      2\n45      1\n46      1\n48      1\n51      1\nName: count, dtype: int64\n\nProportions:\n doubles\n0     0.514173\n1     0.074016\n2     0.036220\n3     0.023622\n4     0.020472\n5     0.018898\n6     0.017323\n14    0.016535\n18    0.015748\n9     0.014173\n13    0.013386\n8     0.012598\n23    0.012598\n10    0.012598\n16    0.011811\n11    0.011811\n12    0.011811\n22    0.011811\n25    0.011024\n7     0.011024\n15    0.010236\n28    0.009449\n17    0.009449\n26    0.007874\n21    0.007087\n19    0.007087\n27    0.006299\n20    0.006299\n31    0.006299\n35    0.005512\n30    0.005512\n32    0.005512\n33    0.004724\n34    0.004724\n29    0.004724\n24    0.004724\n38    0.003150\n42    0.003150\n36    0.003150\n40    0.002362\n41    0.001575\n44    0.001575\n43    0.001575\n37    0.001575\n47    0.001575\n45    0.000787\n46    0.000787\n48    0.000787\n51    0.000787\nName: proportion, dtype: float64\n\n--------------------------------------------------\n\nAnalysis for triples:\n\nUnique Levels: [ 0  1  5  2  6  7  4  3 10  9  8 12]\n\nCounts:\n triples\n0     941\n1     128\n2      81\n3      48\n4      24\n5      15\n6      12\n7       9\n8       6\n9       3\n10      2\n12      1\nName: count, dtype: int64\n\nProportions:\n triples\n0     0.740945\n1     0.100787\n2     0.063780\n3     0.037795\n4     0.018898\n5     0.011811\n6     0.009449\n7     0.007087\n8     0.004724\n9     0.002362\n10    0.001575\n12    0.000787\nName: proportion, dtype: float64\n\n--------------------------------------------------\n\nAnalysis for HR:\n\nUnique Levels: [ 0  2  3 32  1 43 36 15 13  4 14 39 23 17 24 10  9 12  5  7 37 27 38 11\n 26 30 22 29 34 16 33 21  6 31 25 20  8 35 19 18 28 48 40]\n\nCounts:\n HR\n0     734\n1      92\n2      42\n3      35\n4      31\n6      28\n5      22\n9      22\n11     21\n8      18\n10     16\n7      16\n13     15\n15     14\n12     13\n16     13\n17     12\n20     12\n14     12\n21     11\n23     11\n24      9\n22      9\n19      7\n18      7\n27      7\n25      6\n34      4\n26      4\n37      3\n32      3\n30      3\n38      3\n29      2\n39      2\n36      2\n35      2\n28      2\n31      1\n43      1\n33      1\n48      1\n40      1\nName: count, dtype: int64\n\nProportions:\n HR\n0     0.577953\n1     0.072441\n2     0.033071\n3     0.027559\n4     0.024409\n6     0.022047\n5     0.017323\n9     0.017323\n11    0.016535\n8     0.014173\n10    0.012598\n7     0.012598\n13    0.011811\n15    0.011024\n12    0.010236\n16    0.010236\n17    0.009449\n20    0.009449\n14    0.009449\n21    0.008661\n23    0.008661\n24    0.007087\n22    0.007087\n19    0.005512\n18    0.005512\n27    0.005512\n25    0.004724\n34    0.003150\n26    0.003150\n37    0.002362\n32    0.002362\n30    0.002362\n38    0.002362\n29    0.001575\n39    0.001575\n36    0.001575\n35    0.001575\n28    0.001575\n31    0.000787\n43    0.000787\n33    0.000787\n48    0.000787\n40    0.000787\nName: proportion, dtype: float64\n\n--------------------------------------------------\n\nAnalysis for RBI:\n\nUnique Levels: [  0   1   2   3  21  80   6 130  19 110  36   9  61  14  52  79  15  92\n  98  76  38  70  58  83  60  63  50  12  33   5  55  43  42  22  89 107\n  93  51  40  44  64   7  16  87  75 108  85 111  10   4 103  77  17  41\n  34  67  53  11 104 101  48  35  88  39  31  18  54  68  72  46  74  37\n  62  30  25  65  84  32  73  57  45 105  86  13  99 100  20  71  28  78\n  23  47  29   8  59  27  81  97  49  24  69  95  56 123  96  82  66  26]\n\nCounts:\n RBI\n0      596\n1       73\n2       38\n4       29\n3       24\n      ... \n101      1\n104      1\n111      1\n89       1\n26       1\nName: count, Length: 108, dtype: int64\n\nProportions:\n RBI\n0      0.469291\n1      0.057480\n2      0.029921\n4      0.022835\n3      0.018898\n         ...   \n101    0.000787\n104    0.000787\n111    0.000787\n89     0.000787\n26     0.000787\nName: proportion, Length: 108, dtype: float64\n\n--------------------------------------------------\n\nAnalysis for walks:\n\nUnique Levels: [  0   1   2  81   3  69  14  68  17   8  55  47 122  11  42  76  48  71\n  32  49  61   5  38  37  21  20  22  70  72  25  73  24  45   7  12  35\n  10  79  59  23  29  90  36  16   4  30  96 108  64  51   9  58  31  39\n  67  15  19  62  60  34  52  26 106  33   6  78  18  28  92  80  13  43\n  44  41  77  54  27  40 102  50  95 130  53  63  87  84  83  46  65 110\n  66  74  56]\n\nCounts:\n walks\n0      591\n1       88\n2       51\n3       29\n4       25\n      ... \n108      1\n122      1\n68       1\n81       1\n56       1\nName: count, Length: 93, dtype: int64\n\nProportions:\n walks\n0      0.465354\n1      0.069291\n2      0.040157\n3      0.022835\n4      0.019685\n         ...   \n108    0.000787\n122    0.000787\n68     0.000787\n81     0.000787\n56     0.000787\nName: proportion, Length: 93, dtype: float64\n\n--------------------------------------------------\n\nAnalysis for strike_outs:\n\nUnique Levels: [  0   1   3   5  91   9   2 146  24 135  43  12  79  21  54 124  19 125\n 132  60  82  94  80 104 114  69  47  29  64  11  96  46  40  27 151  97\n 122  72  36  98  93 123  32  44  18  99 168 134  63 167 106 173 115 102\n  50  62  75  85  83  31 148 101  53  41 142  86  59 110  38  16 152  95\n 109  73 107 113  71  77  49   8 140  65  34 143  68 129  22  61 126 108\n  37  13  17  15  45  42 128   4  87 211 119 156  20  55 131 138  66 116\n  84  14  35  88 158 176 100  58  78 111 145  25  30  57 117 178  23 127\n 155  33 103   6 169 121  89  52  67 147 163 175 150 133 159 153 149 217\n 137  10  28  90  74 112 165  76  39 160  48   7  26 207 120  51 192  70]\n\nCounts:\n strike_outs\n0      351\n1      122\n2       52\n3       31\n4       23\n      ... \n155      1\n169      1\n89       1\n147      1\n70       1\nName: count, Length: 162, dtype: int64\n\nProportions:\n strike_outs\n0      0.276378\n1      0.096063\n2      0.040945\n3      0.024409\n4      0.018110\n         ...   \n155    0.000787\n169    0.000787\n89     0.000787\n147    0.000787\n70     0.000787\nName: proportion, Length: 162, dtype: float64\n\n--------------------------------------------------\n\nAnalysis for stolen_bases:\n\nUnique Levels: [ 0 30  3  2  6  7 22  1 17 24  4 10 12 45 20  9 16  8 14 40  5 27 21 23\n 11 33 25 32 15 43 34 28 35 13 19 26]\n\nCounts:\n stolen_bases\n0     862\n1     104\n2      70\n3      45\n4      26\n5      24\n6      23\n7      17\n12     12\n10     12\n8       9\n9       7\n14      7\n11      6\n16      6\n13      5\n15      4\n24      4\n21      4\n30      3\n20      3\n17      2\n34      2\n19      1\n35      1\n28      1\n43      1\n45      1\n32      1\n25      1\n33      1\n23      1\n27      1\n22      1\n40      1\n26      1\nName: count, dtype: int64\n\nProportions:\n stolen_bases\n0     0.678740\n1     0.081890\n2     0.055118\n3     0.035433\n4     0.020472\n5     0.018898\n6     0.018110\n7     0.013386\n12    0.009449\n10    0.009449\n8     0.007087\n9     0.005512\n14    0.005512\n11    0.004724\n16    0.004724\n13    0.003937\n15    0.003150\n24    0.003150\n21    0.003150\n30    0.002362\n20    0.002362\n17    0.001575\n34    0.001575\n19    0.000787\n35    0.000787\n28    0.000787\n43    0.000787\n45    0.000787\n32    0.000787\n25    0.000787\n33    0.000787\n23    0.000787\n27    0.000787\n22    0.000787\n40    0.000787\n26    0.000787\nName: proportion, dtype: float64\n\n--------------------------------------------------\n\nAnalysis for caught_stealing_base:\n\nUnique Levels: [ 0  6  1  4  2  3  7 10 11 12  5  9 14  8]\n\nCounts:\n caught_stealing_base\n0     942\n1     117\n2      70\n3      46\n4      35\n6      18\n5      18\n7       8\n10      4\n12      3\n9       3\n11      2\n14      2\n8       2\nName: count, dtype: int64\n\nProportions:\n caught_stealing_base\n0     0.741732\n1     0.092126\n2     0.055118\n3     0.036220\n4     0.027559\n6     0.014173\n5     0.014173\n7     0.006299\n10    0.003150\n12    0.002362\n9     0.002362\n11    0.001575\n14    0.001575\n8     0.001575\nName: proportion, dtype: float64\n\n--------------------------------------------------\n\nAnalysis for AVG:\n\nUnique Levels: [1.    0.667 0.5   0.429 0.4   0.375 0.355 0.346 0.339 0.333 0.33  0.329\n 0.326 0.322 0.321 0.316 0.314 0.312 0.31  0.309 0.308 0.306 0.305 0.304\n 0.303 0.301 0.3   0.299 0.298 0.297 0.296 0.294 0.293 0.292 0.291 0.29\n 0.288 0.287 0.286 0.285 0.284 0.283 0.282 0.281 0.28  0.279 0.278 0.277\n 0.276 0.275 0.274 0.273 0.272 0.271 0.27  0.269 0.268 0.267 0.266 0.265\n 0.264 0.263 0.262 0.261 0.26  0.259 0.258 0.257 0.256 0.255 0.254 0.253\n 0.252 0.251 0.25  0.249 0.248 0.247 0.246 0.245 0.244 0.243 0.242 0.241\n 0.24  0.239 0.238 0.237 0.236 0.235 0.234 0.233 0.232 0.231 0.23  0.229\n 0.228 0.227 0.226 0.225 0.224 0.223 0.222 0.221 0.22  0.219 0.217 0.216\n 0.215 0.214 0.213 0.212 0.211 0.21  0.209 0.208 0.207 0.206 0.205 0.204\n 0.203 0.202 0.201 0.2   0.199 0.198 0.197 0.196 0.195 0.194 0.192 0.19\n 0.189 0.188 0.187 0.186 0.185 0.184 0.183 0.182 0.181 0.18  0.179 0.178\n 0.177 0.176 0.175 0.174 0.173 0.17  0.169 0.168 0.167 0.165 0.164 0.163\n 0.162 0.161 0.16  0.159 0.158 0.156 0.154 0.152 0.15  0.148 0.147 0.143\n 0.141 0.14  0.138 0.136 0.135 0.134 0.133 0.13  0.128 0.125 0.122 0.12\n 0.119 0.118 0.117 0.116 0.115 0.114 0.111 0.109 0.108 0.105 0.103 0.102\n 0.1   0.095 0.094 0.093 0.092 0.091 0.089 0.088 0.087 0.083 0.08  0.078\n 0.077 0.075 0.07  0.067 0.065 0.064 0.063 0.059 0.057 0.056 0.052 0.048\n 0.045 0.044 0.042 0.038 0.034 0.024 0.019 0.   ]\n\nCounts:\n AVG\n0.000    501\n0.167     20\n0.200     18\n0.250     17\n0.333     14\n        ... \n0.375      1\n0.169      1\n0.170      1\n0.173      1\n0.197      1\nName: count, Length: 224, dtype: int64\n\nProportions:\n AVG\n0.000    0.394488\n0.167    0.015748\n0.200    0.014173\n0.250    0.013386\n0.333    0.011024\n           ...   \n0.375    0.000787\n0.169    0.000787\n0.170    0.000787\n0.173    0.000787\n0.197    0.000787\nName: proportion, Length: 224, dtype: float64\n\n--------------------------------------------------\n\nAnalysis for OBP:\n\nUnique Levels: [1.    0.667 0.5   0.429 0.333 0.375 0.371 0.438 0.4   0.402 0.381 0.398\n 0.394 0.386 0.329 0.406 0.46  0.357 0.388 0.364 0.374 0.395 0.358 0.378\n 0.367 0.341 0.331 0.3   0.417 0.354 0.33  0.405 0.336 0.366 0.328 0.359\n 0.397 0.361 0.349 0.34  0.355 0.352 0.337 0.348 0.323 0.326 0.389 0.338\n 0.36  0.345 0.325 0.339 0.308 0.286 0.342 0.304 0.415 0.376 0.351 0.309\n 0.313 0.335 0.294 0.322 0.392 0.356 0.327 0.362 0.35  0.306 0.288 0.321\n 0.396 0.316 0.273 0.385 0.334 0.303 0.344 0.332 0.314 0.387 0.31  0.29\n 0.319 0.353 0.267 0.343 0.39  0.301 0.346 0.289 0.295 0.377 0.391 0.404\n 0.382 0.324 0.347 0.305 0.312 0.317 0.368 0.318 0.287 0.307 0.297 0.282\n 0.274 0.315 0.25  0.293 0.222 0.393 0.302 0.311 0.299 0.279 0.292 0.281\n 0.235 0.298 0.291 0.28  0.268 0.277 0.269 0.285 0.253 0.275 0.266 0.283\n 0.284 0.278 0.252 0.258 0.276 0.262 0.271 0.225 0.256 0.255 0.263 0.254\n 0.239 0.296 0.242 0.236 0.214 0.211 0.26  0.247 0.264 0.257 0.265 0.272\n 0.261 0.226 0.244 0.248 0.259 0.2   0.24  0.193 0.209 0.221 0.217 0.188\n 0.241 0.207 0.218 0.245 0.232 0.227 0.19  0.191 0.224 0.243 0.143 0.167\n 0.194 0.184 0.161 0.203 0.192 0.174 0.179 0.183 0.154 0.15  0.237 0.246\n 0.178 0.208 0.136 0.175 0.21  0.204 0.171 0.125 0.176 0.163 0.119 0.118\n 0.162 0.156 0.148 0.233 0.115 0.114 0.205 0.111 0.172 0.103 0.131 0.1\n 0.17  0.094 0.093 0.091 0.231 0.089 0.139 0.16  0.083 0.096 0.127 0.077\n 0.075 0.109 0.086 0.067 0.097 0.065 0.102 0.063 0.158 0.123 0.074 0.105\n 0.056 0.052 0.045 0.061 0.038 0.048 0.019 0.    0.095]\n\nCounts:\n OBP\n0.000    474\n0.333     28\n0.250     20\n0.200     19\n0.500     18\n        ... \n0.227      1\n0.232      1\n0.245      1\n0.218      1\n0.095      1\nName: count, Length: 249, dtype: int64\n\nProportions:\n OBP\n0.000    0.373228\n0.333    0.022047\n0.250    0.015748\n0.200    0.014961\n0.500    0.014173\n           ...   \n0.227    0.000787\n0.232    0.000787\n0.245    0.000787\n0.218    0.000787\n0.095    0.000787\nName: proportion, Length: 249, dtype: float64\n\n--------------------------------------------------\n\nAnalysis for SLG:\n\nUnique Levels: [1.    2.    0.833 0.5   0.714 0.4   0.6   1.125 0.516 0.64  0.39  0.333\n 1.333 0.667 0.629 0.471 0.598 0.671 0.452 0.451 0.614 0.518 0.628 0.422\n 0.49  0.505 0.468 0.535 0.417 0.487 0.44  0.457 0.438 0.415 0.474 0.411\n 0.3   0.35  0.435 0.431 0.454 0.448 0.538 0.527 0.561 0.414 0.406 0.366\n 0.552 0.483 0.364 0.45  0.398 0.38  0.517 0.567 0.502 0.428 0.554 0.71\n 0.581 0.465 0.533 0.481 0.522 0.525 0.427 0.479 0.416 0.461 0.532 0.378\n 0.492 0.286 0.564 0.493 0.419 0.372 0.382 0.392 0.512 0.434 0.526 0.47\n 0.379 0.446 0.433 0.42  0.37  0.498 0.528 0.508 0.46  0.407 0.519 0.456\n 0.484 0.467 0.413 0.345 0.464 0.429 0.363 0.539 0.534 0.273 0.384 0.53\n 0.545 0.489 0.344 0.48  0.376 0.444 0.395 0.466 0.346 0.308 0.389 0.349\n 0.494 0.347 0.491 0.396 0.388 0.386 0.267 0.331 0.509 0.449 0.412 0.473\n 0.353 0.375 0.421 0.356 0.292 0.486 0.582 0.453 0.408 0.496 0.458 0.597\n 0.477 0.436 0.357 0.403 0.52  0.377 0.437 0.523 0.463 0.343 0.381 0.367\n 0.425 0.439 0.424 0.469 0.394 0.332 0.499 0.32  0.362 0.404 0.462 0.426\n 0.459 0.307 0.25  0.34  0.55  0.625 0.336 0.504 0.476 0.409 0.432 0.549\n 0.397 0.279 0.326 0.305 0.271 0.325 0.418 0.385 0.283 0.405 0.374 0.373\n 0.355 0.342 0.327 0.368 0.324 0.359 0.289 0.297 0.266 0.391 0.317 0.315\n 0.304 0.291 0.313 0.262 0.328 0.33  0.387 0.338 0.423 0.351 0.36  0.393\n 0.281 0.354 0.278 0.348 0.288 0.314 0.369 0.335 0.321 0.478 0.217 0.371\n 0.216 0.312 0.214 0.309 0.383 0.242 0.339 0.399 0.263 0.244 0.239 0.231\n 0.253 0.264 0.241 0.302 0.29  0.258 0.205 0.303 0.41  0.2   0.24  0.272\n 0.8   0.316 0.213 0.232 0.28  0.365 0.268 0.301 0.211 0.259 0.257 0.188\n 0.306 0.185 0.294 0.269 0.298 0.284 0.176 0.223 0.261 0.26  0.247 0.296\n 0.167 0.319 0.238 0.274 0.204 0.179 0.163 0.27  0.194 0.16  0.158 0.246\n 0.154 0.152 0.175 0.148 0.235 0.143 0.224 0.254 0.203 0.138 0.136 0.135\n 0.165 0.222 0.17  0.125 0.122 0.169 0.116 0.115 0.114 0.111 0.105 0.172\n 0.128 0.119 0.1   0.19  0.094 0.093 0.123 0.159 0.091 0.089 0.088 0.13\n 0.083 0.08  0.098 0.077 0.103 0.075 0.067 0.097 0.065 0.064 0.063 0.059\n 0.057 0.056 0.069 0.048 0.045 0.038 0.034 0.024 0.019 0.   ]\n\nCounts:\n SLG\n0.000    501\n0.333     19\n0.250     17\n0.200     15\n0.500     14\n        ... \n0.549      1\n0.397      1\n0.418      1\n0.405      1\n0.625      1\nName: count, Length: 346, dtype: int64\n\nProportions:\n SLG\n0.000    0.394488\n0.333    0.014961\n0.250    0.013386\n0.200    0.011811\n0.500    0.011024\n           ...   \n0.549    0.000787\n0.397    0.000787\n0.418    0.000787\n0.405    0.000787\n0.625    0.000787\nName: proportion, Length: 346, dtype: float64\n\n--------------------------------------------------\n\nAnalysis for OPS:\n\nUnique Levels: [2.    3.    1.667 1.5   1.    1.167 1.143 0.733 1.1   0.887 1.078 0.765\n 0.667 0.762 1.067 1.031 0.852 1.069 0.846 0.837 0.943 0.924 1.088 0.797\n 0.847 0.892 0.832 0.909 0.813 0.845 0.817 0.821 0.806 0.755 0.805 0.6\n 0.65  0.789 0.804 0.836 0.79  0.843 0.854 0.905 0.855 0.935 0.83  0.811\n 0.773 0.727 0.917 0.703 0.868 0.729 0.754 0.728 0.923 0.914 0.86  0.751\n 0.881 1.043 0.922 0.818 0.883 0.874 0.803 0.742 0.785 0.926 0.701 0.792\n 0.8   0.571 0.925 0.859 0.714 0.741 0.696 0.89  0.849 0.732 0.758 0.664\n 0.838 0.704 0.919 0.864 0.787 0.774 0.747 0.871 0.798 0.825 0.796 0.763\n 0.888 0.633 0.749 0.794 0.678 0.757 0.545 0.93  0.834 0.824 0.722 0.82\n 0.679 0.76  0.776 0.731 0.78  0.939 0.668 0.629 0.699 0.637 0.829 0.744\n 0.697 0.676 0.736 0.801 0.533 0.672 0.74  0.786 0.743 0.705 0.677 0.651\n 0.81  0.735 0.623 0.756 0.973 0.886 0.863 0.723 0.95  0.768 0.75  0.682\n 0.764 0.715 0.814 0.882 0.706 0.709 0.619 0.809 0.71  0.769 0.753 0.897\n 0.808 0.654 0.85  0.675 0.693 0.777 0.73  0.669 0.782 0.719 0.708 0.639\n 0.793 0.839 0.775 0.816 0.626 0.718 0.72  0.643 0.702 0.64  0.68  0.738\n 0.5   0.9   0.583 0.656 0.472 0.657 0.958 0.778 0.889 0.645 0.662 0.833\n 0.724 0.7   0.779 0.624 0.788 0.592 0.694 0.713 0.617 0.815 0.746 0.681\n 0.865 0.687 0.771 0.684 0.622 0.823 0.634 0.661 0.69  0.576 0.614 0.635\n 0.688 0.671 0.613 0.566 0.567 0.717 0.602 0.581 0.557 0.761 0.608 0.597\n 0.683 0.642 0.604 0.711 0.593 0.653 0.649 0.546 0.767 0.673 0.766 0.644\n 0.655 0.648 0.611 0.565 0.605 0.59  0.627 0.555 0.475 0.559 0.61  0.541\n 0.652 0.526 0.591 0.575 0.658 0.609 0.783 0.467 0.584 0.58  0.485 0.618\n 0.589 0.429 0.712 0.558 0.674 0.621 0.474 0.685 0.504 0.523 0.478 0.63\n 0.54  0.529 0.499 0.691 0.574 0.552 0.553 0.519 0.603 0.46  0.599 0.692\n 0.51  0.551 0.4   0.544 0.473 1.229 0.435 0.739 0.588 0.577 0.547 0.425\n 0.663 0.486 0.433 0.564 0.539 0.528 0.423 0.438 0.532 0.501 0.399 0.631\n 0.582 0.455 0.516 0.628 0.698 0.421 0.578 0.511 0.562 0.549 0.477 0.44\n 0.471 0.452 0.464 0.615 0.31  0.333 0.56  0.417 0.394 0.492 0.42  0.487\n 0.39  0.363 0.454 0.355 0.382 0.352 0.378 0.337 0.383 0.462 0.368 0.34\n 0.325 0.327 0.495 0.357 0.36  0.286 0.47  0.392 0.393 0.432 0.439 0.299\n 0.403 0.273 0.406 0.451 0.388 0.426 0.52  0.376 0.313 0.25  0.301 0.285\n 0.288 0.294 0.373 0.272 0.264 0.269 0.314 0.319 0.311 0.222 0.395 0.444\n 0.424 0.341 0.296 0.276 0.328 0.267 0.268 0.2   0.289 0.346 0.188 0.219\n 0.186 0.242 0.53  0.258 0.209 0.322 0.182 0.178 0.227 0.29  0.35  0.208\n 0.195 0.194 0.243 0.154 0.185 0.174 0.133 0.18  0.191 0.323 0.158 0.166\n 0.156 0.259 0.213 0.179 0.15  0.161 0.139 0.121 0.128 0.248 0.091 0.132\n 0.124 0.077 0.131 0.072 0.038 0.    0.095 0.1   0.167 0.125 0.083]\n\nCounts:\n OPS\n0.000    474\n0.500     17\n0.333     11\n0.667     11\n1.000     10\n        ... \n0.683      1\n0.604      1\n0.711      1\n0.593      1\n0.083      1\nName: count, Length: 443, dtype: int64\n\nProportions:\n OPS\n0.000    0.373228\n0.500    0.013386\n0.333    0.008661\n0.667    0.008661\n1.000    0.007874\n           ...   \n0.683    0.000787\n0.604    0.000787\n0.711    0.000787\n0.593    0.000787\n0.083    0.000787\nName: proportion, Length: 443, dtype: float64\n\n--------------------------------------------------\n\n```\n:::\n:::\n\n\n## Describe\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nmlb_players_18.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>games</th>\n      <th>AB</th>\n      <th>R</th>\n      <th>H</th>\n      <th>doubles</th>\n      <th>triples</th>\n      <th>HR</th>\n      <th>RBI</th>\n      <th>walks</th>\n      <th>strike_outs</th>\n      <th>stolen_bases</th>\n      <th>caught_stealing_base</th>\n      <th>AVG</th>\n      <th>OBP</th>\n      <th>SLG</th>\n      <th>OPS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1270.000000</td>\n      <td>1270.000000</td>\n      <td>1270.000000</td>\n      <td>1270.000000</td>\n      <td>1270.000000</td>\n      <td>1270.000000</td>\n      <td>1270.000000</td>\n      <td>1270.000000</td>\n      <td>1270.000000</td>\n      <td>1270.000000</td>\n      <td>1270.000000</td>\n      <td>1270.000000</td>\n      <td>1270.000000</td>\n      <td>1270.000000</td>\n      <td>1270.000000</td>\n      <td>1270.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>48.171654</td>\n      <td>130.261417</td>\n      <td>17.031496</td>\n      <td>32.297638</td>\n      <td>6.507087</td>\n      <td>0.666929</td>\n      <td>4.397638</td>\n      <td>16.225197</td>\n      <td>12.351181</td>\n      <td>32.446457</td>\n      <td>1.948031</td>\n      <td>0.754331</td>\n      <td>0.140191</td>\n      <td>0.181824</td>\n      <td>0.217412</td>\n      <td>0.399239</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>49.957749</td>\n      <td>185.855484</td>\n      <td>26.896304</td>\n      <td>49.396815</td>\n      <td>10.487391</td>\n      <td>1.517461</td>\n      <td>8.036863</td>\n      <td>26.085535</td>\n      <td>20.680606</td>\n      <td>44.687302</td>\n      <td>5.018058</td>\n      <td>1.769933</td>\n      <td>0.140268</td>\n      <td>0.165976</td>\n      <td>0.218611</td>\n      <td>0.374984</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>5.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>29.000000</td>\n      <td>23.500000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>8.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.166000</td>\n      <td>0.217500</td>\n      <td>0.214000</td>\n      <td>0.436500</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>79.750000</td>\n      <td>213.750000</td>\n      <td>27.000000</td>\n      <td>50.000000</td>\n      <td>10.000000</td>\n      <td>1.000000</td>\n      <td>5.000000</td>\n      <td>24.000000</td>\n      <td>18.000000</td>\n      <td>54.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.247000</td>\n      <td>0.316000</td>\n      <td>0.395000</td>\n      <td>0.703000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>162.000000</td>\n      <td>664.000000</td>\n      <td>129.000000</td>\n      <td>192.000000</td>\n      <td>51.000000</td>\n      <td>12.000000</td>\n      <td>48.000000</td>\n      <td>130.000000</td>\n      <td>130.000000</td>\n      <td>217.000000</td>\n      <td>45.000000</td>\n      <td>14.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>3.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n:::\n\n## Preprocessing {.smaller}\n\n::: panel-tabset\n## Define columns\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# Define the columns based on their type for preprocessing\ncategorical_features = ['team', 'position']\nnumerical_features = ['games', 'AB', 'R', 'H', 'doubles', 'triples', 'HR', 'RBI', 'walks', 'strike_outs', 'stolen_bases', 'caught_stealing_base', 'AVG', 'OBP', 'SLG', 'OPS']\n```\n:::\n\n\n## Prepare steps\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Handling missing values: Impute missing values if any\n# For numerical features, replace missing values with the median of the column\n# For categorical features, replace missing values with the most frequent value of the column\nnumerical_transformer = Pipeline(steps = [\n    ('imputer', SimpleImputer(strategy = 'median')),\n    ('scaler', StandardScaler())])\n\ncategorical_transformer = Pipeline(steps = [\n    ('onehot', OneHotEncoder(handle_unknown = 'ignore'))])\n\npreprocessor = ColumnTransformer(transformers = [\n    ('num', numerical_transformer, numerical_features),\n    ('cat', categorical_transformer, categorical_features)])\n```\n:::\n\n\n## Transformations\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# Apply the transformations to the dataset\nmlb_preprocessed = preprocessor.fit_transform(mlb_players_18)\n\n# The result is a NumPy array. To convert it back to a DataFrame:\n# Update the method to get_feature_names_out for compatibility with newer versions of scikit-learn\nfeature_names = list(preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features))\nnew_columns = numerical_features + feature_names\n\nmlb_preprocessed_df = pd.DataFrame(mlb_preprocessed, columns = new_columns)\nmlb_preprocessed_df.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>games</th>\n      <th>AB</th>\n      <th>R</th>\n      <th>H</th>\n      <th>doubles</th>\n      <th>triples</th>\n      <th>HR</th>\n      <th>RBI</th>\n      <th>walks</th>\n      <th>strike_outs</th>\n      <th>...</th>\n      <th>position_1B</th>\n      <th>position_2B</th>\n      <th>position_3B</th>\n      <th>position_C</th>\n      <th>position_CF</th>\n      <th>position_DH</th>\n      <th>position_LF</th>\n      <th>position_P</th>\n      <th>position_RF</th>\n      <th>position_SS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.904553</td>\n      <td>-0.695768</td>\n      <td>-0.596283</td>\n      <td>-0.633846</td>\n      <td>-0.620712</td>\n      <td>-0.439676</td>\n      <td>-0.547399</td>\n      <td>-0.622245</td>\n      <td>-0.59747</td>\n      <td>-0.726364</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.944603</td>\n      <td>-0.690386</td>\n      <td>-0.559089</td>\n      <td>-0.613594</td>\n      <td>-0.620712</td>\n      <td>-0.439676</td>\n      <td>-0.547399</td>\n      <td>-0.622245</td>\n      <td>-0.59747</td>\n      <td>-0.726364</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.824454</td>\n      <td>-0.695768</td>\n      <td>-0.596283</td>\n      <td>-0.633846</td>\n      <td>-0.620712</td>\n      <td>-0.439676</td>\n      <td>-0.547399</td>\n      <td>-0.622245</td>\n      <td>-0.59747</td>\n      <td>-0.726364</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.944603</td>\n      <td>-0.690386</td>\n      <td>-0.633478</td>\n      <td>-0.613594</td>\n      <td>-0.620712</td>\n      <td>-0.439676</td>\n      <td>-0.547399</td>\n      <td>-0.583894</td>\n      <td>-0.59747</td>\n      <td>-0.726364</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.884529</td>\n      <td>-0.695768</td>\n      <td>-0.596283</td>\n      <td>-0.633846</td>\n      <td>-0.525322</td>\n      <td>-0.439676</td>\n      <td>-0.547399</td>\n      <td>-0.622245</td>\n      <td>-0.59747</td>\n      <td>-0.726364</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 56 columns</p>\n</div>\n```\n:::\n:::\n\n\n:::\n\n# Before moving on: <br> Similarity / Dissimilarity\n\n## Similarity + Dissimilarity {style=\"text-align: center;\"}\n\n![](images/sim-dissim.webp){fig-align=\"center\" width=\"567\"}\n\n## Similarity {.smaller}\n\n::: panel-tabset\n## Cosine\n\n![](images/cosine.jpg){fig-align=\"center\" width=\"795\"}\n\n$\\cos(\\theta) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|} = \\frac{\\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\sqrt{\\sum_{i=1}^{n} B_i^2}}$\n\n::: incremental\n-   Best for text data or any high-dimensional data.\n\n-   Useful when the magnitude of the data vector is not important.\n\n-   [Python](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html)\n:::\n\n## Jaccard\n\n![](images/jaccard.jpg){fig-align=\"center\" width=\"712\"}\n\n$J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}$\n\n::: incremental\n-   Suitable for sets or binary data.\n\n-   Ideal for comparing the similarity between two sample sets.\n\n-   [Python](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html)\n:::\n\n## Pearson $r$\n\n![](images/pearson.png){fig-align=\"center\" width=\"657\"}\n\n$r = \\frac{\\sum (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sqrt{\\sum (X_i - \\bar{X})^2 \\sum (Y_i - \\bar{Y})^2}}$\n\n::: incremental\n-   Use when measuring the linear relationship between two continuous variables.\n\n-   Appropriate for data with a normal distribution.\n\n-   [Python](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html)\n:::\n\n## Spearman $\\rho$\n\n::: columns\n::: {.column width=\"33.33%\"}\n![](images/spearman-1.png){fig-align=\"center\" width=\"370\"}\n:::\n\n::: {.column width=\"33.33%\"}\n![](images/spearman-2.png){fig-align=\"center\" width=\"370\"}\n:::\n\n::: {.column width=\"33.33%\"}\n![](images/spearman-3.png){fig-align=\"center\" width=\"370\"}\n:::\n:::\n\n$\\rho = 1 - \\frac{6 \\sum d_i^2}{n(n^2 - 1)}$\n\n::: incremental\n-   Ordinal data or when data do not meet the assumptions of Pearson's correlation.\n\n-   Monotonic relationships between two continuous or ordinal variables.\n\n-   [Python](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html)\n:::\n:::\n\n## Dissimilarity {.smaller}\n\n::: panel-tabset\n## Euclidean\n\n![](images/euclidean.png){fig-align=\"center\" width=\"363\"}\n\n$d(\\mathbf{p}, \\mathbf{q}) = \\sqrt{\\sum_{i=1}^{n} (p_i - q_i)^2}$\n\n::: incremental\n-   Use for continuous data to measure the \"straight line\" distance between points in Euclidean space.\n-   Most common in clustering and classification where simple distance measurement is required.\n-   [Python](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.euclidean.html)\n:::\n\n## Manhattan\n\n![](images/manhattan.png){fig-align=\"center\" width=\"250\"}\n\n$d(\\mathbf{p}, \\mathbf{q}) = \\sum_{i=1}^{n} |p_i - q_i|$\n\n::: incremental\n-   Suitable for continuous or ordinal data where you want to measure the distance as if navigating a grid-like path (like city blocks).\n-   Useful when the difference across dimensions is important regardless of the path taken.\n-   [Python](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.manhattan_distances.html)\n:::\n\n## Hamming\n\n![](images/hamming.jpeg){fig-align=\"center\" width=\"362\"}\n\n$d(\\mathbf{p}, \\mathbf{q}) = \\sum_{i=1}^{n} \\delta(p_i, q_i) \\quad \\text{where} \\quad \\delta(a, b) = \\begin{cases} 1 & \\text{if } a \\neq b \\\\ 0 & \\text{otherwise} \\end{cases}$\n\n::: incremental\n-   Use for categorical or binary data.\n-   Ideal for comparing two strings of equal length or binary feature vectors.\n-   [Python](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.hamming.html)\n:::\n\n## Minkowski\n\n![](images/minkowski.png){fig-align=\"center\" width=\"1209\"}\n\n$d(\\mathbf{p}, \\mathbf{q}) = \\left( \\sum_{i=1}^{n} |p_i - q_i|^p \\right)^{\\frac{1}{p}}$\n\n::: incremental\n-   A generalization of Euclidean and Manhattan distances. Use when you need to fine-tune the distance calculation by emphasizing different dimensions.\n-   Parameterizable for different applications; adjust the parameter to control the impact of different dimensions.\n-   [Python](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.minkowski.html)\n:::\n\n## Mahalanobis\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell execution_count=9}\n\n::: {.cell-output .cell-output-display}\n![](11-unsupervised-1_files/figure-revealjs/cell-10-output-1.png){width=516 height=381}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n$$d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{(\\mathbf{x} - \\mathbf{y})^T S^{-1} (\\mathbf{x} - \\mathbf{y})}$$\n\n::: incremental\n-   Best for multivariate data where variables are correlated or scales differ.\n\n-   Useful in identifying outliers or in clustering when data is not isotropic.\n\n-   [Python](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.mahalanobis.html)\n:::\n:::\n:::\n:::\n\n# Clustering\n\n## Clustering methods\n\n::: {style=\"text-align: center;\"}\n\n```{=html}\n<iframe width=\"1200\" height=\"400\" src=\"https://datamineaz.org/tables/model-cheatsheet.html\" frameborder=\"1\" style=\"background:white;\"></iframe>\n```\n\n:::\n\n## K-Means Clustering {.smaller}\n\n::: panel-tabset\n## Visual\n\n::: {.cell execution_count=10}\n\n::: {.cell-output .cell-output-display}\n![](11-unsupervised-1_files/figure-revealjs/cell-11-output-1.png){width=793 height=435}\n:::\n:::\n\n\n## Formula\n\n> The goal of K-Means is to minimize the variance within each cluster. The variance is measured as the sum of squared distances between each point and its corresponding cluster centroid. The objective function, which K-Means aims to minimize, can be defined as:\n\n$J = \\sum_{i=1}^{k} \\sum_{x \\in C_i} ||x - \\mu_i||^2$\n\n**Where**:\n\n::: incremental\n-   $J$ is the objective function\n\n-   $k$ is the number of clusters\n\n-   $C_i$ is the set of points belonging to a cluster $i$.\n\n-   $x$ is a point in the cluster $C_i$\n\n-   $||x - \\mu_i||^2$ is the squared Euclidean distance between a point $x$ and the centroid $\\mu_i$â€‹, which measures the dissimilarity between them.\n:::\n\n## Key points\n\n-   **Initialization**: Randomly selects $k$ initial centroids.\n\n-   **Assignment Step**: Assigns each data point to the closest centroid based on Euclidean distance.\n\n-   **Update Step**: Recalculates centroids as the mean of assigned points in each cluster.\n\n-   **Convergence**: Iterates until the centroids stabilize (minimal change from one iteration to the next).\n\n-   **Objective**: Minimizes the within-cluster sum of squares (WCSS), the sum of squared distances between points and their corresponding centroid.\n\n-   **Optimal** $k$: Determined experimentally, often using methods like the Elbow Method.\n\n-   **Sensitivity**: Results can vary based on initial centroid selection; techniques like \"k-means++\" improve initial centroid choices.\n\n-   **Efficiency**: Generally good, but worsens with increasing $k$ and data dimensionality; sensitive to outliers.\n:::\n\n## K-Medians Clustering {.smaller}\n\n::: panel-tabset\n## Visual\n\n::: {.cell execution_count=11}\n\n::: {.cell-output .cell-output-display}\n![](11-unsupervised-1_files/figure-revealjs/cell-12-output-1.png){width=793 height=435}\n:::\n:::\n\n\n## Formula\n\n$\\min \\sum_{i=1}^{k} \\sum_{x \\in C_i} ||x - m_i||_1$\n\n-   $k$ is the number of clusters.\n\n-   $C_i$â€‹ represents the data points in cluster $i$.\n\n-   $x$ is a point within cluster $C_i$â€‹.\n\n-   $m_i$â€‹ is the median of the data points in cluster $i$, replacing the mean from K-Means.\n\n-   $||x - \\mu_i||_1$ denotes the Manhattan distance (L1 norm) between point $x$ and median $m_i$â€‹.\n\n## Key points\n\n::: incremental\n-   **Initialization**: Randomly selects $k$ initial medians.\n\n-   **Assignment Step**: Assigns each data point to the closest median based on some distance metric, typically Manhattan distance.\n\n-   **Update Step**: Recalculates medians as the median of assigned points in each cluster.\n\n-   **Convergence**: Iterates until the medians stabilize (minimal change from one iteration to the next).\n\n-   **Objective**: Minimizes the within-cluster sum of absolute deviations (WSAD), the sum of absolute differences between points and their corresponding median.\n\n-   **Optimal** $k$: Determined experimentally, often using methods like the Elbow Method.\n\n-   **Sensitivity**: Results can vary based on initial median selection; techniques like \"k-medians++\" may improve initial choices.\n\n-   **Efficiency**: Generally good, but can worsen with increasing $k$ and data dimensionality; often more robust to outliers compared to k-means.\n:::\n:::\n\n## K-Means vs. K-Medians clustering {.smaller}\n\n**K-Means Clustering**:\n\n::: incremental\n-   Groups data by minimizing the variance within clusters.\n\n-   Adopts the mean as the cluster center.\n\n-   Prone to the impact of outliers.\n\n-   Effective for locations in high-dimensional spaces and for \"spherical\" cluster shapes.\n:::\n\n**K-Median Clustering**:\n\n::: incremental\n-   Prioritizes the minimization of the sum of absolute deviations.\n\n-   Adopts the median as the cluster center.\n\n-   More robust to outliers than K-Means.\n\n-   Bets for non-spherical data, and effectively manages the distortion in distributions.\n:::\n\n## Choosing the right number of clusters {.smaller}\n\n**Four main methods:**\n\n::: incremental\n-   **Elbow Method**\n\n    -   Identifies the $k$ at which the within-cluster sum of squares (WCSS) starts to diminish more slowly.\n\n-   **Silhouette Score**\n\n    -   Measures how similar an object is to its own cluster compared to other clusters.\n\n-   **Davies-Bouldin Index**\n\n    -   Evaluates intra-cluster similarity and inter-cluster differences.\n\n-   **Calinski-Harabasz Index (Variance Ratio Criterion)**\n\n    -   Measures the ratio of the sum of between-clusters dispersion and of intra-cluster dispersion for all clusters.\n\n-   **BIC**\n\n    -   Identifies the optimal number of clusters by penalizing models for excessive parameters, striking a balance between simplicity and accuracy.\n:::\n\n## Elbow Method {.smaller}\n\n::: panel-tabset\n## Visual\n\n::: {.cell execution_count=12}\n\n::: {.cell-output .cell-output-display}\n![](11-unsupervised-1_files/figure-revealjs/cell-13-output-1.png){width=828 height=529}\n:::\n:::\n\n\n## Pros + Cons\n\n**Pros:**\n\n::: incremental\n-   **Simple and easy to understand**: Requires minimal statistical knowledge.\n\n-   **Clear graphical representation**: Helps intuitively identify the optimal number of clusters.\n\n-   **Versatile**: Applicable to various clustering algorithms.\n:::\n\n**Cons:**\n\n::: incremental\n-   **Subjective**: The \"elbow\" point can be ambiguous, leading to different interpretations.\n\n-   **Not ideal for all datasets**: Difficulty in identifying a clear elbow in datasets with gradual variance reduction.\n\n-   **Computationally expensive**: For large datasets, calculating WCSS for many values of $k$ can be resource-intensive.\n\n-   **Sensitive to initialization**: The initial placement of centroids can influence the identification of the elbow point.\n:::\n:::\n\n## Silhouette Score {.smaller}\n\n::: panel-tabset\n## Visual\n\n::: {.cell execution_count=13}\n\n::: {.cell-output .cell-output-stdout}\n```\nFor 3 clusters Silhouetter Score: 0.553\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](11-unsupervised-1_files/figure-revealjs/cell-14-output-2.png){width=646 height=447}\n:::\n:::\n\n\n## Formula\n\n$s(i) = \\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}}$\n\n::: incremental\n-   $a$ is the mean distance between a sample and all other points in the same class.\n\n-   $b$ is the mean distance between a sample and all other points in the next nearest cluster.\n:::\n\n## Pros + Cons\n\n**Pros:**\n\n::: incremental\n-   The score provides insight into the distance between the resulting clusters.\n\n-   Values range from -1 to 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n:::\n\n**Cons:**\n\n::: incremental\n-   Computationally expensive for large datasets.\n\n-   Does not perform well with clusters of varying densities.\n:::\n:::\n\n## Davies-Bouldin Index {.smaller}\n\n::: panel-tabset\n## Visual\n\n::: {.cell execution_count=14}\n\n::: {.cell-output .cell-output-display}\n![](11-unsupervised-1_files/figure-revealjs/cell-15-output-1.png){width=821 height=533}\n:::\n:::\n\n\n## Formula\n\n$DBI = \\frac{1}{k} \\sum_{i=1}^{k} \\max_{j \\neq i} \\left( \\frac{\\sigma_i + \\sigma_j}{d(c_i, c_j)} \\right)$\n\n**Where**:\n\n::: incremental\n-   $k$ is the number of clusters\n\n-   $\\sigma_i$ is the average distance of all points in cluster $i$ to the centroid of cluster $i$ (intra-cluster distance)\n\n-   $d(c_i, c_j)$ is the distance between centroids $i$ and $j$\n\n-   The ratio $\\frac{\\sigma_i + \\sigma_j}{d(c_i, c_j)}$ reflects the similarity between clusters $i$ and $j$, with lower values indicating clusters are well-separated and compact.\n:::\n\n## Pros + Cons\n\n**Pros:**\n\n::: incremental\n-   **Intuitive**: Easy to understand and interpret. A lower DBI value means better clustering.\n\n-   **Versatile**: Applicable to any distance metric used within the clustering algorithm.\n\n-   **Useful for Comparing Models**: Effective for comparing the performance of different clustering models on the same dataset.\n:::\n\n**Cons:**\n\n::: incremental\n-   **Sensitivity to Cluster Density**: May not perform well with clusters of varying densities, as it relies on the mean distances within clusters.\n\n-   **Does Not Scale Well**: Computationally expensive for large datasets due to the calculation of distances between all pairs of clusters.\n\n-   **Ambiguity in Interpretation**: While lower values are better, there's no clear threshold below which clusters are considered 'good' or 'optimal'.\n:::\n:::\n\n## Calinski-Harabasz Index {.smaller}\n\n::: panel-tabset\n## Visual\n\n::: {.cell execution_count=15}\n\n::: {.cell-output .cell-output-display}\n![](11-unsupervised-1_files/figure-revealjs/cell-16-output-1.png){width=833 height=533}\n:::\n:::\n\n\n## Formula\n\n$CH = \\frac{SS_B / (k - 1)}{SS_W / (n - k)}$\n\n**where**:\n\n::: incremental\n-   $CH$ is the Calinski-Harabasz score.\n\n-   $SS_B$â€‹ is the between-cluster variance.\n\n-   $SS_W$â€‹ is the within-cluster variance.\n\n-   $k$ is the number of clusters.\n\n-   $n$ is the number of data points.\n:::\n\n## Pros + cons\n\n**Pros**:\n\n::: incremental\n-   **Clear Interpretation**: High values indicate better-defined clusters.\n\n-   **Computationally Efficient**: Less resource-intensive than many alternatives.\n\n-   **Scale-Invariant**: Effective across datasets of varying sizes.\n\n-   **No Labeled Data Required**: Useful for unsupervised learning scenarios.\n:::\n\n**Cons**:\n\n::: incremental\n-   **Cluster Structure Bias**: Prefers convex clusters of similar sizes.\n\n-   **Sample Size Sensitivity**: Can favor more clusters in larger datasets.\n\n-   **Not Ideal for Overlapping Clusters**: Assumes distinct, non-overlapping clusters.\n:::\n:::\n\n## BIC {.smaller}\n\n::: panel-tabset\n## Visual\n\n::: {.cell execution_count=16}\n\n::: {.cell-output .cell-output-display}\n![](11-unsupervised-1_files/figure-revealjs/cell-17-output-1.png){width=833 height=533}\n:::\n:::\n\n\n## Formula\n\n$\\text{BIC} = -2 \\ln(\\hat{L}) + k \\ln(n)$\n\n**where**:\n\n::: incremental\n-   $\\hat{L}$ is the maximized value of the likelihood function of the model,\n\n-   $k$ is the number of parameters in the model,\n\n-   $n$ is the number of observations.\n:::\n\n## Pros + cons\n\n**Pros**:\n\n-   **Penalizes Complexity**: Helps avoid overfitting by penalizing models with more parameters.\n\n-   **Objective Selection**: Facilitates choosing the model with the best balance between fit and simplicity.\n\n-   **Applicability**: Useful across various model types, including clustering and regression.\n\n**Cons**:\n\n-   **Computationally Intensive**: Requires fitting multiple models to calculate, which can be resource-heavy.\n\n-   **Sensitivity to Model Assumptions**: Performance depends on the underlying assumptions of the model being correct.\n\n-   **Not Always Intuitive**: Determining the absolute best model may still require domain knowledge and additional diagnostics.\n:::\n\n## Systematic comparison: Equal clusters {.smaller}\n\n::: panel-tabset\n## Elbow\n\n![](images/equally-sized-elbow.png){fig-align=\"center\" width=\"896\"}\n\n## Davies-Boulin\n\n![](images/equally-sized-davies-bouldin.png){fig-align=\"center\" width=\"896\"}\n\n## Silhouette\n\n![](images/equally-sized-silhouette.png){fig-align=\"center\" width=\"896\"}\n\n## Calinski-Harabasz\n\n![](images/equally-sized-calinski-harabasz.png){fig-align=\"center\" width=\"896\"}\n\n## BIC\n\n![](images/equally-sized-bic.png){fig-align=\"center\" width=\"896\"}\n:::\n\n## Systematic comparison: Unequal clusters {.smaller}\n\n::: panel-tabset\n## Elbow\n\n![](images/unequally-sized-elbow.png){fig-align=\"center\" width=\"896\"}\n\n## Davies-Boulin\n\n![](images/unequally-sized-davies-bouldin.png){fig-align=\"center\" width=\"896\"}\n\n## Silhouette\n\n![](images/unequally-sized-silhouette.png){fig-align=\"center\" width=\"896\"}\n\n## Calinski-Harabasz\n\n![](images/unequally-sized-calinski-harabasz.png){fig-align=\"center\" width=\"896\"}\n\n## BIC\n\n![](images/unequally-sized-bic.png){fig-align=\"center\" width=\"896\"}\n:::\n\n## Systematic comparison - accuracy\n\n![](images/systematic-comparison.png){fig-align=\"center\" width=\"692\"}\n\n## K-Means Clustering: applied {.smaller}\n\n::: panel-tabset\n## Model summary\n\n::: {.cell execution_count=17}\n``` {.python .cell-code code-fold=\"true\"}\n# K-Means Clustering\nkmeans = KMeans(n_clusters = 5, random_state = 0)  # Adjust n_clusters as needed\nkmeans.fit(mlb_preprocessed_df)\nclusters = kmeans.predict(mlb_preprocessed_df)\n\n# Adding cluster labels to the DataFrame\nmlb_preprocessed_df['Cluster'] = clusters\n\n# Evaluate clustering performance\nsilhouette_avg = silhouette_score(mlb_preprocessed, clusters)\nprint(\"For n_clusters =\", 5, f\"The average silhouette_score is : {silhouette_avg:.3f}\")\nprint(\"\")\n\n# Model Summary\nprint(\"Cluster Centers:\\n\", kmeans.cluster_centers_)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFor n_clusters = 5 The average silhouette_score is : 0.361\n\nCluster Centers:\n [[ 9.80447852e-01  8.68398274e-01  6.85686523e-01  7.80094891e-01\n   7.12100496e-01  3.88054976e-01  5.27932579e-01  6.95521803e-01\n   6.66153895e-01  9.08848347e-01  3.11613648e-01  4.56004838e-01\n   7.26773039e-01  7.81705387e-01  7.87473189e-01  8.05195721e-01\n   3.33333333e-02  3.33333333e-02  5.00000000e-02  3.33333333e-02\n   2.77777778e-02  1.66666667e-02  3.88888889e-02  2.77777778e-02\n   2.77777778e-02  3.33333333e-02  2.77777778e-02  3.88888889e-02\n   2.77777778e-02  1.11111111e-02  1.66666667e-02  3.88888889e-02\n   5.55555556e-02  3.88888889e-02  3.33333333e-02  3.33333333e-02\n   4.44444444e-02  3.33333333e-02  3.88888889e-02  2.77777778e-02\n   5.55555556e-02  3.88888889e-02  3.33333333e-02  2.77777778e-02\n   3.33333333e-02  2.22222222e-02  1.33333333e-01  1.11111111e-01\n   1.16666667e-01  2.00000000e-01  1.16666667e-01  1.11111111e-02\n   9.44444444e-02 -6.66133815e-16  1.33333333e-01  8.33333333e-02]\n [-6.43923908e-01 -6.68405672e-01 -6.24289850e-01 -6.44989752e-01\n  -6.14049361e-01 -4.39676439e-01 -5.45490433e-01 -6.14666152e-01\n  -5.88322957e-01 -6.56038799e-01 -3.86998693e-01 -4.25396567e-01\n  -8.91273096e-01 -9.35527561e-01 -9.09541581e-01 -9.44343098e-01\n   4.08858603e-02  4.94037479e-02  2.55536627e-02  3.23679727e-02\n   4.25894378e-02  4.42930153e-02  2.55536627e-02  3.40715503e-02\n   2.38500852e-02  4.25894378e-02  2.55536627e-02  2.55536627e-02\n   2.89608177e-02  4.42930153e-02  3.06643952e-02  3.57751278e-02\n   2.04429302e-02  4.25894378e-02  2.55536627e-02  3.23679727e-02\n   3.06643952e-02  3.23679727e-02  3.57751278e-02  2.72572402e-02\n   3.57751278e-02  3.91822828e-02  2.72572402e-02  2.72572402e-02\n   3.23679727e-02  3.91822828e-02  1.70357751e-03  8.51788756e-03\n   5.11073254e-03  1.19250426e-02  1.02214651e-02  8.67361738e-19\n   1.70357751e-03  9.48892675e-01  6.81431005e-03  5.11073254e-03]\n [-3.91402474e-01 -3.79744257e-01 -3.95022196e-01 -3.90508911e-01\n  -3.87210975e-01 -2.73851534e-01 -3.87795143e-01 -3.94376804e-01\n  -3.70441099e-01 -3.43732560e-01 -2.64216595e-01 -2.85922460e-01\n   7.27521486e-01  7.47585532e-01  6.23908840e-01  6.94635443e-01\n   2.76073620e-02  3.37423313e-02  4.29447853e-02  2.14723926e-02\n   3.06748466e-02  3.37423313e-02  3.06748466e-02  3.06748466e-02\n   1.84049080e-02  2.45398773e-02  1.84049080e-02  3.68098160e-02\n   4.60122699e-02  3.37423313e-02  5.82822086e-02  4.29447853e-02\n   2.14723926e-02  4.60122699e-02  2.76073620e-02  3.06748466e-02\n   3.06748466e-02  3.37423313e-02  3.06748466e-02  3.68098160e-02\n   3.68098160e-02  3.06748466e-02  3.37423313e-02  3.37423313e-02\n   3.98773006e-02  3.68098160e-02  3.68098160e-02  1.04294479e-01\n   7.66871166e-02  1.99386503e-01  8.89570552e-02 -9.54097912e-18\n   7.66871166e-02  2.57668712e-01  7.66871166e-02  8.28220859e-02]\n [ 1.84336601e+00  2.00093650e+00  2.01422924e+00  2.01298538e+00\n   2.03251916e+00  9.00100073e-01  2.18403901e+00  2.20827125e+00\n   2.01821145e+00  1.90441535e+00  5.34481046e-01  6.63047958e-01\n   8.63620542e-01  9.34621852e-01  1.09476387e+00  1.05182158e+00\n   5.64516129e-02  3.22580645e-02  1.61290323e-02  4.03225806e-02\n   5.64516129e-02  3.22580645e-02  3.22580645e-02  2.41935484e-02\n   2.41935484e-02  2.41935484e-02  5.64516129e-02  1.61290323e-02\n   3.22580645e-02  8.06451613e-02  3.22580645e-02  4.03225806e-02\n   2.41935484e-02  1.61290323e-02  4.03225806e-02  4.03225806e-02\n   4.83870968e-02  1.61290323e-02  1.61290323e-02  3.22580645e-02\n   1.61290323e-02  4.03225806e-02  1.61290323e-02  4.03225806e-02\n   3.22580645e-02  2.41935484e-02  1.69354839e-01  8.87096774e-02\n   1.77419355e-01  5.64516129e-02  8.06451613e-02  3.22580645e-02\n   1.53225806e-01  8.06451613e-03  1.61290323e-01  7.25806452e-02]\n [ 1.89665173e+00  2.10798003e+00  2.30278073e+00  2.18655890e+00\n   2.00881676e+00  3.13025217e+00  1.52406415e+00  1.70483602e+00\n   1.81025384e+00  1.83795064e+00  3.60257994e+00  3.37018281e+00\n   9.07494502e-01  9.21541805e-01  1.00020250e+00  9.90889402e-01\n   1.88679245e-02  5.66037736e-02  1.88679245e-02  5.66037736e-02\n   1.88679245e-02  3.77358491e-02  3.77358491e-02  5.66037736e-02\n   7.54716981e-02  3.77358491e-02  1.88679245e-02  3.77358491e-02\n   1.88679245e-02  1.88679245e-02 -2.08166817e-17  3.77358491e-02\n   0.00000000e+00  3.77358491e-02  5.66037736e-02  1.88679245e-02\n   1.88679245e-02  3.77358491e-02  5.66037736e-02  5.66037736e-02\n   0.00000000e+00  0.00000000e+00  7.54716981e-02  1.88679245e-02\n   1.88679245e-02  5.66037736e-02  1.88679245e-02  2.45283019e-01\n   3.77358491e-02  1.38777878e-17  2.45283019e-01 -8.67361738e-19\n   1.50943396e-01  1.11022302e-16  5.66037736e-02  2.45283019e-01]]\n```\n:::\n:::\n\n\n## Visualize results\n\n::: {.cell execution_count=18}\n``` {.python .cell-code code-fold=\"true\"}\npca = PCA(n_components = 2)\nmlb_pca = pca.fit_transform(mlb_preprocessed)\nsns.scatterplot(x = mlb_pca[:, 0], y = mlb_pca[:, 1], hue = clusters, alpha = 0.75, palette = \"colorblind\")\nplt.title('MLB Players Clustered (PCA-reduced Features)')\nplt.xlabel('PCA 1')\nplt.ylabel('PCA 2')\nplt.legend(title = 'Cluster')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](11-unsupervised-1_files/figure-revealjs/cell-19-output-1.png){width=668 height=496}\n:::\n:::\n\n\n:::\n\n## Live coding: apply K-Medians Clustering {.smaller}\n\n::: {.cell execution_count=19}\n``` {.python .cell-code code-fold=\"true\"}\nfrom sklearn_extra.cluster import KMedoids\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\n\n# Using 'mlb_preprocessed_df' is your DataFrame after preprocessing\ndata = mlb_preprocessed_df.to_numpy()\n\n# Create KMedoids instance with 5 clusters and Manhattan distance (L1)\nkmedoids_instance = KMedoids(n_clusters = 5, metric = 'manhattan', random_state = 42)\n\n# Fit the model and predict cluster labels\ncluster_labels = kmedoids_instance.fit_predict(data)\n\n# Assign cluster labels to each record in DataFrame\nmlb_preprocessed_df['Cluster'] = cluster_labels\n\n# Evaluate clustering performance using silhouette score\nsilhouette_avg = silhouette_score(data, cluster_labels)\nprint(f\"For n_clusters = 5, The average silhouette_score is : {silhouette_avg:.3f}\")\n\n# Displaying the medians (centroids) of the clusters\n# Note: KMedoids uses actual data points as centers, not the mean or median of the cluster.\nprint(\"Cluster Medians (Centers):\\n\", kmedoids_instance.cluster_centers_)\n\n# Visualize using PCA for dimensionality reduction\npca = PCA(n_components = 2)\nmlb_pca = pca.fit_transform(data)\n\n# Plotting the clusters\nsns.scatterplot(x = mlb_pca[:, 0], y = mlb_pca[:, 1], hue = cluster_labels, alpha = 0.75, palette = \"colorblind\")\nplt.title('MLB Players Clustered (PCA-reduced Features)')\nplt.xlabel('PCA 1')\nplt.ylabel('PCA 2')\nplt.legend(title = 'Cluster')\nplt.show()\n```\n:::\n\n\n## Conclusions {.smaller}\n\n::: incremental\n-   **Unsupervised Learning**: Explores data to find structure in the form of clusters without predefined labels or outcomes.\n\n-   **Clustering Use Cases**: Includes recommender systems, anomaly detection, genetics, and customer segmentation.\n\n-   **Clustering Algorithms**: K-Means and K-Medians are highlighted for their utility in grouping data based on similarity measures.\n\n-   **Choosing the Right Number of Clusters**: Techniques like the Elbow Method, Silhouette Score, Davies-Bouldin Index, Calinski-Harabasz Index, and BIC are critical for determining the optimal cluster count.\n\n-   **Similarity and Dissimilarity Measures**: Essential in clustering, with methods including Euclidean, Manhattan, Cosine, Jaccard, and Mahalanobis distances.\n\n-   **Evaluation Metrics**: Silhouette Score, Davies-Bouldin Index, and Calinski-Harabasz Index help assess clustering quality, focusing on intra-cluster cohesion and inter-cluster separation.\n:::\n\n",
    "supporting": [
      "11-unsupervised-1_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}